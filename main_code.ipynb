{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6vYeiUzfChZ",
        "outputId": "71bca1c2-197d-48df-a893-73432a0eb1a3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zR2GujjMfCd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "ScDjp9qWfCbX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports and Dataset Setup"
      ],
      "metadata": {
        "id": "xhTrAh9y6_LR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SentenceDataset(Dataset):\n",
        "    def __init__(self, inputs, labels):\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.labels[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    forward_inputs = [item[0][0] for item in batch]\n",
        "    backward_inputs = [item[0][1] for item in batch]\n",
        "    labels = [item[1] for item in batch]\n",
        "\n",
        "    # Pad sequences\n",
        "    forward_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(x) for x in forward_inputs], batch_first=True)\n",
        "    backward_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(x) for x in backward_inputs], batch_first=True)\n",
        "\n",
        "    return (forward_padded, backward_padded), torch.tensor(labels)\n"
      ],
      "metadata": {
        "id": "QWSIi3zSfCYu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_fill_in_the_blank(sentence):\n",
        "    words = sentence.split()\n",
        "    if len(words) < 6:\n",
        "        return None\n",
        "\n",
        "    # Enhanced cleaning\n",
        "    words = [word.strip().lower() for word in words\n",
        "            if len(word.strip()) > 2 and word.isalnum()]\n",
        "\n",
        "    if len(words) < 6:\n",
        "        return None\n",
        "\n",
        "    # More strategic split point selection\n",
        "    split_point = len(words) // 2\n",
        "\n",
        "    # Avoid splitting in the middle of phrases\n",
        "    while split_point > 0 and words[split_point].lower() in {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at'}:\n",
        "        split_point -= 1\n",
        "\n",
        "    latter_half = words[split_point:]\n",
        "    if len(latter_half) < 2:\n",
        "        return None\n",
        "\n",
        "    # More strategic blank selection\n",
        "    valid_positions = [\n",
        "        i for i, word in enumerate(latter_half)\n",
        "        if len(word) > 2 and word not in {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at'}\n",
        "    ]\n",
        "\n",
        "    if not valid_positions:\n",
        "        return None\n",
        "\n",
        "    blank_index = random.choice(valid_positions)\n",
        "    blank_word = latter_half.pop(blank_index)\n",
        "\n",
        "    # Create parts with more context\n",
        "    part_a = \" \".join(words[:split_point])\n",
        "    part_b = \" \".join(latter_half[::-1])\n",
        "\n",
        "    return part_a, part_b, blank_word"
      ],
      "metadata": {
        "id": "44bZRVKZfCWI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PREPROCESS DATA"
      ],
      "metadata": {
        "id": "e3xvvgCG7Mg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_dataset(dataset, tokenizer, max_samples=2000):\n",
        "    print(\"Processing sentences...\")\n",
        "    sentences = []\n",
        "    for item in tqdm(dataset[\"train\"]):\n",
        "        for sentence in item[\"article\"].split(\".\"):\n",
        "            sentence = sentence.strip()\n",
        "            if len(sentence) > 10:\n",
        "                result = create_fill_in_the_blank(sentence)\n",
        "                if result:\n",
        "                    sentences.append(result)\n",
        "\n",
        "    sentences = sentences[:max_samples]\n",
        "    print(f\"Total processed sentences: {len(sentences)}\")\n",
        "\n",
        "    # Split data\n",
        "    train_data, val_data = train_test_split(sentences, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Tokenize data\n",
        "    def tokenize_data(data):\n",
        "        inputs, labels = [], []\n",
        "        for part_a, part_b, blank_word in tqdm(data):\n",
        "            try:\n",
        "                part_a_tokens = tokenizer.encode(part_a, truncation=True, max_length=50)\n",
        "                part_b_tokens = tokenizer.encode(part_b, truncation=True, max_length=50)\n",
        "                label_token = tokenizer.encode(blank_word, add_special_tokens=False)[0]\n",
        "                inputs.append((part_a_tokens, part_b_tokens))\n",
        "                labels.append(label_token)\n",
        "            except Exception as e:\n",
        "                print(f\"Error tokenizing: {str(e)}\")\n",
        "                continue\n",
        "        return inputs, labels\n",
        "\n",
        "    train_inputs, train_labels = tokenize_data(train_data)\n",
        "    val_inputs, val_labels = tokenize_data(val_data)\n",
        "\n",
        "    return train_inputs, train_labels, val_inputs, val_labels\n"
      ],
      "metadata": {
        "id": "kP4rJj3KjHWk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BiDirectionalLSTM"
      ],
      "metadata": {
        "id": "rqdpctAa7Rdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BiDirectionalLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(BiDirectionalLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Upgraded to multi-layer LSTM\n",
        "        self.forward_lstm = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=2,\n",
        "            dropout=0.3,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.backward_lstm = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=2,\n",
        "            dropout=0.3,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Add attention mechanism\n",
        "        self.forward_attention = nn.MultiheadAttention(hidden_dim * 2, num_heads=4, dropout=0.1)\n",
        "        self.backward_attention = nn.MultiheadAttention(hidden_dim * 2, num_heads=4, dropout=0.1)\n",
        "\n",
        "        # Enhanced output layers\n",
        "        self.forward_fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "        self.backward_fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "        # Enhanced confidence scorer\n",
        "        self.confidence_scorer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
        "            nn.LayerNorm(hidden_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
        "\n",
        "    def forward(self, forward_x, backward_x, training=True):\n",
        "        # Enhanced embedding with dropout\n",
        "        forward_embedded = self.dropout(self.embedding(forward_x))\n",
        "        backward_embedded = self.dropout(self.embedding(backward_x))\n",
        "\n",
        "        # Bi-directional LSTM processing\n",
        "        forward_output, (forward_hidden, _) = self.forward_lstm(forward_embedded)\n",
        "        backward_output, (backward_hidden, _) = self.backward_lstm(backward_embedded)\n",
        "\n",
        "        # Apply attention\n",
        "        forward_output = forward_output.transpose(0, 1)  # [batch, seq, hidden] -> [seq, batch, hidden]\n",
        "        backward_output = backward_output.transpose(0, 1)\n",
        "\n",
        "        forward_attended, _ = self.forward_attention(\n",
        "            forward_output, forward_output, forward_output\n",
        "        )\n",
        "        backward_attended, _ = self.backward_attention(\n",
        "            backward_output, backward_output, backward_output\n",
        "        )\n",
        "\n",
        "        # Get final hidden states\n",
        "        forward_hidden = torch.cat([forward_hidden[-2], forward_hidden[-1]], dim=1)\n",
        "        backward_hidden = torch.cat([backward_hidden[-2], backward_hidden[-1]], dim=1)\n",
        "\n",
        "        # Apply layer normalization\n",
        "        forward_hidden = self.layer_norm(forward_hidden)\n",
        "        backward_hidden = self.layer_norm(backward_hidden)\n",
        "\n",
        "        # Get predictions\n",
        "        forward_logits = self.forward_fc(forward_attended[-1])\n",
        "        backward_logits = self.backward_fc(backward_attended[-1])\n",
        "\n",
        "        # Calculate confidence with enhanced hidden states\n",
        "        confidence = self.confidence_scorer(\n",
        "            torch.cat([forward_hidden, backward_hidden], dim=1)\n",
        "        )\n",
        "\n",
        "        if training:\n",
        "            return forward_logits, backward_logits, confidence\n",
        "        else:\n",
        "            return self.select_prediction(forward_logits, backward_logits, confidence)\n",
        "\n",
        "    def select_prediction(self, forward_logits, backward_logits, confidence):\n",
        "        # Enhanced prediction selection\n",
        "        forward_probs = F.softmax(forward_logits, dim=1)\n",
        "        backward_probs = F.softmax(backward_logits, dim=1)\n",
        "\n",
        "        # Temperature scaling for sharper distributions\n",
        "        temperature = 0.7\n",
        "        forward_probs = forward_probs.pow(1/temperature)\n",
        "        backward_probs = backward_probs.pow(1/temperature)\n",
        "\n",
        "        # Normalize\n",
        "        forward_probs = forward_probs / forward_probs.sum(dim=1, keepdim=True)\n",
        "        backward_probs = backward_probs / backward_probs.sum(dim=1, keepdim=True)\n",
        "\n",
        "        # Weighted average with enhanced confidence\n",
        "        weighted_probs = (forward_probs * confidence +\n",
        "                         backward_probs * (1 - confidence))\n",
        "\n",
        "        return weighted_probs\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs):\n",
        "    best_accuracy = 0\n",
        "    patience = 3\n",
        "    patience_counter = 0\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        for batch_inputs, batch_labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "            forward_inputs, backward_inputs = batch_inputs\n",
        "            forward_inputs = forward_inputs.to(device)\n",
        "            backward_inputs = backward_inputs.to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get predictions and confidence\n",
        "            forward_logits, backward_logits, confidence = model(\n",
        "                forward_inputs, backward_inputs, training=True\n",
        "            )\n",
        "\n",
        "            # Calculate losses with label smoothing\n",
        "            forward_loss = criterion(forward_logits, batch_labels)\n",
        "            backward_loss = criterion(backward_logits, batch_labels)\n",
        "\n",
        "            # Dynamic confidence weighting\n",
        "            confidence_weight = confidence.mean()\n",
        "            loss = (forward_loss * confidence_weight +\n",
        "                   backward_loss * (1 - confidence_weight))\n",
        "\n",
        "            # Add L2 regularization\n",
        "            l2_lambda = 0.01\n",
        "            l2_reg = torch.tensor(0., device=device)\n",
        "            for param in model.parameters():\n",
        "                l2_reg += torch.norm(param)\n",
        "            loss += l2_lambda * l2_reg\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(forward_logits, 1)\n",
        "            correct_predictions += (predicted == batch_labels).sum().item()\n",
        "            total_predictions += batch_labels.size(0)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_inputs, batch_labels in tqdm(val_loader, desc=\"Validation\"):\n",
        "                forward_inputs, backward_inputs = batch_inputs\n",
        "                forward_inputs = forward_inputs.to(device)\n",
        "                backward_inputs = backward_inputs.to(device)\n",
        "                batch_labels = batch_labels.to(device)\n",
        "\n",
        "                outputs = model(forward_inputs, backward_inputs, training=False)\n",
        "                _, predicted = outputs.max(1)\n",
        "\n",
        "                val_total += batch_labels.size(0)\n",
        "                val_correct += predicted.eq(batch_labels).sum().item()\n",
        "\n",
        "                # Calculate validation loss\n",
        "                forward_logits, backward_logits, _ = model(\n",
        "                    forward_inputs, backward_inputs, training=True\n",
        "                )\n",
        "                val_loss += criterion(forward_logits, batch_labels).item()\n",
        "\n",
        "        val_accuracy = 100. * val_correct / val_total\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}\")\n",
        "        print(f\"Training Loss: {total_loss/len(train_loader):.4f}\")\n",
        "        print(f\"Training Accuracy: {100. * correct_predictions/total_predictions:.2f}%\")\n",
        "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "        print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "        # Early stopping with patience\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "            if val_accuracy > best_accuracy:\n",
        "                best_accuracy = val_accuracy\n",
        "                torch.save(model.state_dict(), 'best_model.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n",
        "                break\n",
        "\n",
        "        scheduler.step(val_accuracy)\n"
      ],
      "metadata": {
        "id": "xFYBaJvEfCTv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load dataset and tokenizer\n",
        "    print(\"Loading dataset...\")\n",
        "    dataset = load_dataset(\"race\", \"all\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    # Process and prepare data\n",
        "    train_inputs, train_labels, val_inputs, val_labels = process_dataset(\n",
        "        dataset, tokenizer, max_samples=2000\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_dataset = SentenceDataset(train_inputs, train_labels)\n",
        "    val_dataset = SentenceDataset(val_inputs, val_labels)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=16, collate_fn=collate_fn)\n",
        "\n",
        "    # Initialize model\n",
        "    model = BiDirectionalLSTM(\n",
        "        vocab_size=tokenizer.vocab_size,\n",
        "        embedding_dim=256,\n",
        "        hidden_dim=512,\n",
        "        output_dim=tokenizer.vocab_size\n",
        "    ).to(device)\n",
        "\n",
        "    # Training setup\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='max', patience=2, factor=0.5\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Starting training...\")\n",
        "    train_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        device=device,\n",
        "        num_epochs=5\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZQCnpASjQp0",
        "outputId": "16eed873-20a2-4e9e-b5a1-e1ec4ef81657"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing sentences...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 87866/87866 [00:26<00:00, 3376.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total processed sentences: 2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1600/1600 [00:00<00:00, 4640.31it/s]\n",
            "100%|██████████| 400/400 [00:00<00:00, 4876.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 100/100 [03:31<00:00,  2.11s/it]\n",
            "Validation: 100%|██████████| 25/25 [00:17<00:00,  1.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n",
            "Training Loss: 46.4921\n",
            "Training Accuracy: 0.62%\n",
            "Validation Loss: 9.1086\n",
            "Validation Accuracy: 2.25%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 100/100 [03:30<00:00,  2.10s/it]\n",
            "Validation: 100%|██████████| 25/25 [00:18<00:00,  1.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2\n",
            "Training Loss: 42.9491\n",
            "Training Accuracy: 1.38%\n",
            "Validation Loss: 8.3264\n",
            "Validation Accuracy: 2.25%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 100/100 [03:31<00:00,  2.11s/it]\n",
            "Validation: 100%|██████████| 25/25 [00:18<00:00,  1.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3\n",
            "Training Loss: 40.7458\n",
            "Training Accuracy: 1.25%\n",
            "Validation Loss: 8.3140\n",
            "Validation Accuracy: 0.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 100/100 [03:30<00:00,  2.11s/it]\n",
            "Validation: 100%|██████████| 25/25 [00:18<00:00,  1.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4\n",
            "Training Loss: 39.4026\n",
            "Training Accuracy: 2.38%\n",
            "Validation Loss: 8.3003\n",
            "Validation Accuracy: 2.50%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 100/100 [03:29<00:00,  2.09s/it]\n",
            "Validation: 100%|██████████| 25/25 [00:18<00:00,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5\n",
            "Training Loss: 38.4406\n",
            "Training Accuracy: 3.81%\n",
            "Validation Loss: 8.2911\n",
            "Validation Accuracy: 2.25%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from typing import Tuple, List\n",
        "\n",
        "def load_trained_model(model_path: str, device: torch.device) -> Tuple[BiDirectionalLSTM, AutoTokenizer]:\n",
        "    \"\"\"Load the trained model and tokenizer.\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    model = BiDirectionalLSTM(\n",
        "        vocab_size=tokenizer.vocab_size,\n",
        "        embedding_dim=256,\n",
        "        hidden_dim=512,\n",
        "        output_dim=tokenizer.vocab_size\n",
        "    ).to(device)\n",
        "\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def prepare_sentence(sentence: str, blank_position: int, tokenizer: AutoTokenizer) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Prepare a sentence for prediction by creating a blank at the specified position.\"\"\"\n",
        "    words = sentence.split()\n",
        "    if blank_position >= len(words):\n",
        "        raise ValueError(\"Blank position is out of range\")\n",
        "\n",
        "    # Store the actual word and replace it with [MASK]\n",
        "    actual_word = words[blank_position]\n",
        "    words[blank_position] = \"[MASK]\"\n",
        "\n",
        "    # Split into two parts at the mask\n",
        "    part_a = \" \".join(words[:blank_position+1])\n",
        "    part_b = \" \".join(words[blank_position+1:])[::-1]  # Reverse the second part\n",
        "\n",
        "    # Tokenize\n",
        "    forward_tokens = torch.tensor([tokenizer.encode(part_a, truncation=True, max_length=50)])\n",
        "    backward_tokens = torch.tensor([tokenizer.encode(part_b, truncation=True, max_length=50)])\n",
        "\n",
        "    return forward_tokens, backward_tokens, actual_word\n",
        "\n",
        "def predict_blank(model: BiDirectionalLSTM,\n",
        "                 tokenizer: AutoTokenizer,\n",
        "                 forward_tokens: torch.Tensor,\n",
        "                 backward_tokens: torch.Tensor,\n",
        "                 top_k: int = 5) -> List[str]:\n",
        "    \"\"\"Predict the most likely words for the blank.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        outputs = model(forward_tokens, backward_tokens, training=False)\n",
        "        probs, indices = torch.topk(outputs, k=top_k)\n",
        "\n",
        "        # Convert to words\n",
        "        predictions = []\n",
        "        for idx, prob in zip(indices[0], probs[0]):\n",
        "            word = tokenizer.decode([idx])\n",
        "            predictions.append((word, prob.item()))\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def test_model():\n",
        "    # Setup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model_path = 'best_model.pth'\n",
        "    model, tokenizer = load_trained_model(model_path, device)\n",
        "\n",
        "    # Test sentences\n",
        "    test_sentences = [\n",
        "        (\"The students studied hard for their final exam\", 5),  # \"final\"\n",
        "        (\"She walked quickly through the busy street to reach her destination\", 7),  # \"street\"\n",
        "        (\"The chef prepared a delicious meal for his guests\", 5),  # \"meal\"\n",
        "        (\"The sun was setting behind the tall mountains\", 6),  # \"mountains\"\n",
        "        (\"Children played happily in the green park\", 6)  # \"park\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\nTesting model with example sentences:\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for sentence, blank_pos in test_sentences:\n",
        "        print(f\"\\nOriginal sentence: {sentence}\")\n",
        "        print(f\"Position of blank: {blank_pos}\")\n",
        "\n",
        "        try:\n",
        "            # Prepare the sentence\n",
        "            forward_tokens, backward_tokens, actual_word = prepare_sentence(sentence, blank_pos, tokenizer)\n",
        "            forward_tokens = forward_tokens.to(device)\n",
        "            backward_tokens = backward_tokens.to(device)\n",
        "\n",
        "            # Get predictions\n",
        "            predictions = predict_blank(model, tokenizer, forward_tokens, backward_tokens)\n",
        "\n",
        "            # Print results\n",
        "            print(\"\\nPredicted words (with confidence scores):\")\n",
        "            for word, confidence in predictions:\n",
        "                print(f\"  {word}: {confidence:.4f}\")\n",
        "            print(f\"Actual word was: {actual_word}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing sentence: {str(e)}\")\n",
        "\n",
        "        print(\"-\"*50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66bInXynfbPb",
        "outputId": "730fbbc7-6114-439c-b957-c39475222676"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-a8e5a7c0e99a>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing model with example sentences:\n",
            "==================================================\n",
            "\n",
            "Original sentence: The students studied hard for their final exam\n",
            "Position of blank: 5\n",
            "\n",
            "Predicted words (with confidence scores):\n",
            "  that: 0.0444\n",
            "  not: 0.0314\n",
            "  are: 0.0272\n",
            "  her: 0.0269\n",
            "  was: 0.0229\n",
            "Actual word was: their\n",
            "--------------------------------------------------\n",
            "\n",
            "Original sentence: She walked quickly through the busy street to reach her destination\n",
            "Position of blank: 7\n",
            "\n",
            "Predicted words (with confidence scores):\n",
            "  that: 0.0309\n",
            "  are: 0.0297\n",
            "  should: 0.0272\n",
            "  not: 0.0260\n",
            "  they: 0.0245\n",
            "Actual word was: to\n",
            "--------------------------------------------------\n",
            "\n",
            "Original sentence: The chef prepared a delicious meal for his guests\n",
            "Position of blank: 5\n",
            "\n",
            "Predicted words (with confidence scores):\n",
            "  that: 0.0437\n",
            "  for: 0.0318\n",
            "  her: 0.0271\n",
            "  was: 0.0252\n",
            "  are: 0.0230\n",
            "Actual word was: meal\n",
            "--------------------------------------------------\n",
            "\n",
            "Original sentence: The sun was setting behind the tall mountains\n",
            "Position of blank: 6\n",
            "\n",
            "Predicted words (with confidence scores):\n",
            "  are: 0.0445\n",
            "  not: 0.0363\n",
            "  that: 0.0351\n",
            "  about: 0.0278\n",
            "  her: 0.0226\n",
            "Actual word was: tall\n",
            "--------------------------------------------------\n",
            "\n",
            "Original sentence: Children played happily in the green park\n",
            "Position of blank: 6\n",
            "\n",
            "Predicted words (with confidence scores):\n",
            "  are: 0.0391\n",
            "  that: 0.0357\n",
            "  her: 0.0270\n",
            "  not: 0.0243\n",
            "  should: 0.0234\n",
            "Actual word was: park\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VxKw49iQnzUd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}